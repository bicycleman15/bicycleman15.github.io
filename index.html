<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jatin Prakash</title>
  
  <meta name="author" content="Jatin Prakash">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Jatin Prakash</name>
              </p>

              <p>I'm a <del>first</del> second year CS PhD student at New York University advised by <a target="_blank" href="https://rajesh-lab.github.io/">Prof. Rajesh Ranganath</a>. I am also part of the <a target="_blank" href="https://wp.nyu.edu/cilvr/">CILVR Lab</a>.

                <p>I am broadly interested in designing <i>scalable, practical and efficient</i> <strong>foundation model architectures</strong> and <strong>training algorithms:</strong> both pre-training and post-training.

                <p>I believe to accomplish this, one must tackle every part of the modeling stack. 
                  To this end, I have worked on: <strong>data</strong> curation [<a target="_blank" href="https://spiffy-airbus-472.notion.site/What-Can-You-Do-When-You-Have-Zero-Rewards-During-RL-260429bdb7308024b6bdd3ed4f64c15f">1</a>, <a target="_blank" href="https://arxiv.org/abs/2408.09585">2</a>], designing novel <strong>model architectures</strong> [<a target="_blank" href="https://arxiv.org/abs/2511.05313">3</a>], improving <strong>training algorithms</strong> [<a target="_blank" href="https://arxiv.org/abs/2510.20817">4</a>], and low-level <strong>system optimizations</strong> [<a target="_blank" href="https://github.com/microsoft/renee/blob/main/Renee-Camera-Ready.pdf">5</a>].
              
              <p>Previously, I spent two <i>amazing</i> years at <a target="_blank" href="https://www.microsoft.com/en-us/research/lab/microsoft-research-india/">Microsoft Research</a>, where I was advised by <a target="_blank" href="http://manikvarma.org/">Dr. Manik Varma</a> and <a target="_blank" href="http://www.amitsharma.in/">Dr. Amit Sharma</a>. Some of my research during this time found its way into Microsoft Bing [<a target="_blank" href="https://arxiv.org/abs/2408.09585">2</a>, <a target="_blank" href="https://github.com/microsoft/renee/blob/main/Renee-Camera-Ready.pdf">5</a>].
              <p>
                Even before that, I graduated from <a target="_blank" href="https://home.iitd.ac.in/">IIT Delhi</a> with a bachelors in CS. During my undergrad, I worked with <a target="_blank" href="https://www.cse.iitd.ac.in/~chetan/">Prof. Chetan Arora</a>.
              </p>
              <p style="text-align:center">
                <!-- <a target="_blank" href="mailto:jatinprakash1511@gmail.com">Email</a> &nbsp/&nbsp -->
                <a target="_blank" href="mailto:jatin.prakash@nyu.edu">Email</a> &nbsp/&nbsp
                <a target="_blank" href="data/jatin-cv-2.pdf">CV</a> &nbsp/&nbsp
                <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                <a target="_blank" href="https://scholar.google.com/citations?user=NeBK8VMAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a target="_blank" href="https://twitter.com/bicycleman15">Twitter</a> &nbsp/&nbsp
                <a target="_blank" href="https://github.com/bicycleman15/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a target="_blank" href="images/IMG_2680.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/IMG_2680.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <!-- <td style="padding:20px;width:100%;vertical-align:middle"> -->
              <!-- <heading>Research Interests</heading> -->
              <!-- <p> -->
                <!-- I am interested in developing <strong>practical methods</strong> that make machine learning (ML) systems <strong>robust</strong>, especially to <em>naturally</em> occurring distribution changes, so that they can be deployed <em>reliably</em> in the real world at <strong>large-scales</strong>. -->
                <!-- Representative papers are <span class="highlight">highlighted</span>. -->
              <!-- </p> -->
            <!-- </td> -->
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
					
					<heading style="width:200%;vertical-align:middle;font-size:25px">Selected Research</heading><br>


          <!-- Paper -->
					<tr>
            <td style="padding-top:30px;padding-left:30px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/trade_offs.png' width="180" height="75">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Attention and Compression is all you need for Controllably Efficient Language Models</papertitle>
              <br>
              <strong>Jatin Prakash</strong>,
              <a target="_blank" style="color:black">Aahlad Puli</a>, 
							<a target="_blank" style="color:black">Rajesh Ranganath</a>
              <br><em>preprint</em> 2025 <strong><a target="_blank" style="color:red">(New!)</a></strong>
              <br><em>ICML 2025 Efficient Systems for Foundation Models III (ES-FoMo)</em> workshop
              <br>
              <a target="_blank" href="https://arxiv.org/abs/2511.05313">Paper</a> /
              <a target="_blank" href="https://x.com/bicycleman15/status/1987895472409673972?s=20">Tweet</a> /
              <a target="_blank" href="https://github.com/rajesh-lab/cat-transformer">Code</a>
              <p>
              <strong>tldr;</strong> We design a language modeling architecture that gives you a ‚Äúknob‚Äù to control efficiency at test-time, allowing a single model to span the whole spectrum of full-to-efficient without any re-training -- trading-off quality for efficiency directly at test-time. This is unlike prior works that require training from scratch to target particular quality-efficiency trade-offs.
              <!-- The proposed adaptive model outperforms efficient baselines across varying compute-memory budgets, all using a single model only. -->
              <!-- Key idea is to model chunks of tokens given compressed representations of past chunks in the sequence so far. Compression results in a reduced sequence length, offering upto <b>3x</b> speedups and <b>9x</b> memory reductions. -->
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding-top:30px;padding-left:30px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/zero_rewards.webp' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>What Can You Do When You Have Zero Rewards During RL?</papertitle>
              <br>
              <strong>Jatin Prakash*</strong>,
              <a target="_blank" style="color:black">Anirudh Buvanesh*</a>
              <br><em>Blog</em> 2025 <strong><a target="_blank" style="color:red">(New!)</a></strong>
              <br>
              <a target="_blank" href="https://spiffy-airbus-472.notion.site/What-Can-You-Do-When-You-Have-Zero-Rewards-During-RL-260429bdb7308024b6bdd3ed4f64c15f">Blog</a> /
              <a target="_blank" href="https://arxiv.org/abs/2510.03971">arXiv</a> /
              <a target="_blank" href="https://x.com/bicycleman15/status/1966989129444716929?s=20">Tweet 1</a> / <a target="_blank" href="https://x.com/AnirudhBuvanesh/status/1966986349124481152?s=20">Tweet 2</a> /
              <a target="_blank" href="https://github.com/rl4reasoning/rl-baselines">Code (RL reasoning baselines)</a>
              <p>
                <strong>tldr;</strong> We benchmarked recent RL algorithms on a simple star-graph task where they fail in zero reward scenarios, even those specially designed for this case.
                Turns out, a very simple data-centric intervention of just adding easy samples of the task helps unlock RL training. Open-sourced implementations for many RL baselines (that had no official code) for the community to build upon.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding-top:30px;padding-left:30px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/rl_mode_collapse.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>KL-Regularized Reinforcement Learning is Designed to Mode Collapse</papertitle>
              <br>
               <!-- Jeff Guo, Rob Fergus, Rajesh Ranganath -->
              <a target="_blank" style="color:black">Anthony GX-Chen</a>, 
              <strong>Jatin Prakash</strong>,
              <a target="_blank" style="color:black">Jeff Guo</a>, 
							<a target="_blank" style="color:black">Rob Fergus</a>,
              <a target="_blank" style="color:black">Rajesh Ranganath</a>
              <br><em>preprint</em> 2025 <strong><a target="_blank" style="color:red">(New!)</a></strong>
              <br><em>NeurIPS 2025 Foundations of Reasoning in Language Models (RoRLM) </em> workshop
              <br>
              <a target="_blank" href="https://arxiv.org/abs/2510.20817">Paper</a>
              <!-- <a target="_blank" href="https://x.com/bicycleman15/status/1987895472409673972?s=20">Tweet</a> / -->
              <!-- <a target="_blank" href="https://github.com/rajesh-lab/cat-transformer">Code</a> -->
              <p>
                <strong>tldr;</strong> We understand diversity collapse problem in RL, and how to principally fix it in 2 lines of code. Key idea is viewing KL-regularized RL as distribution matching to a target distribution. Our work explores how to define a <i>good</i> target for the proposal distribution (or policy in case of RL) that avoids mode collapse.
              </p>
            </td>
          </tr>

          <!-- Paper -->
					<tr>
            <td style="padding-top:30px;padding-left:30px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/skim_banner.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>On the Necessity of World Knowledge for Mitigating Missing Labels in Extreme Classification</papertitle>
              <br>
              <strong>Jatin Prakash*</strong>,
              <a target="_blank" style="color:black">Anirudh Buvanesh*</a>, 
							<a target="_blank" style="color:black">Bishal Santra</a>, 
              <a target="_blank" style="color:black">Deepak Saini</a>,
              <a target="_blank" style="color:black">Sachin Yadav</a>,
              <a target="_blank" style="color:black">Jian Jiao</a>,
              <a target="_blank" style="color:black">Yashoteja Prabhu</a>,
              <a target="_blank" style="color:black">Amit Sharma</a>,
              <a target="_blank" style="color:black">Manik Varma</a>
              <br><em>KDD</em> 2025
              <br>
              <a target="_blank" href="https://arxiv.org/abs/2408.09585">Paper</a> /
              <a target="_blank" href="https://openreview.net/forum?id=I84W4YjOTD">Reviews</a> /
              <a target="_blank" href="https://github.com/bicycleman15/skim">Code</a>
              <p>
                <strong>tldr;</strong> A simple, scalable and data-centric algorithm to mitigate <i>bad quality</i> click-data problem in retrieval (extreme classification), that scales to real-world industry query-ads datasets containing upto 10M+ documents. This outperforms SOTA significantly, highlighting the importance of <i>good quality</i> dataset (that contains diverse world knowledge) for retrieval. Part of this work has been <b>deployed in Microsoft Bing.</b>
                
              </p>
            </td>
          </tr>

					<!-- Paper -->
          <tr>
            <td style="padding-top:30px;padding-left:30px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/renee-banner.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Renee: End-to-end training of extreme classification models</papertitle>
              <br>
							<!-- Vidit Jain, Jatin Prakash, Deepak Saini, Jian Jiao, Ramachandran Ramjee, Manik Varma -->
              <a target="_blank" style="color:black">Vidit Jain</a>, 
              <strong>Jatin Prakash</strong>,
              <a target="_blank" style="color:black">Deepak Saini</a>, 
							<a target="_blank" style="color:black">Jian Jiao</a>, 
							<a target="_blank" style="color:black">Ramachandran Ramjee</a>, 
							<a target="_blank" style="color:black">Manik Varma</a>
              <br><em>MLSys</em> 2023
              <br>
              <a target="_blank" href="https://github.com/microsoft/renee/blob/main/Renee-Camera-Ready.pdf">Paper</a> /
              <a target="_blank" href="https://github.com/microsoft/renee">Code</a>
              <p> 
                <strong>tldr;</strong> We unlock end-to-end training of large-scale retrieval (extreme classification) models that scales to 100M+ documents and 1B+ training examples, reducing training time from weeks to under a day. Turns out, simple end-to-end learning outperforms complicated, modular SOTA methods. This work has been <b>deployed in Microsoft Bing.</b> 
              </p>
            </td>
          </tr>

          <!-- Paper -->
					<tr>
            <td style="padding-top:30px;padding-left:30px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/lever-banner.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Enhancing Tail Performance in Extreme Classifiers by Label Variance Reduction</papertitle>
              <br>
              <a target="_blank" style="color:black">Anirudh Buvanesh*</a>, 
							<a target="_blank" style="color:black">Rahul Chand*</a>, 
              <strong>Jatin Prakash</strong>,
              <a target="_blank" style="color:black">Bhawna Paliwal</a>,
              <a target="_blank" style="color:black">Mudit Dhawan</a>,
              <a target="_blank" style="color:black">Neelabh Madan</a>,
              <a target="_blank" style="color:black">Deepesh Hada</a>,
              <a target="_blank" style="color:black">Vidit Jain</a>,
              <a target="_blank" style="color:black">Sonu Mehta</a>,
              <a target="_blank" style="color:black">Yashoteja Prabhu</a>,
              <a target="_blank" style="color:black">Manish Gupta</a>,
              <a target="_blank" style="color:black">Ramachandran Ramjee</a>,
							<a target="_blank" style="color:black">Manik Varma</a>
              <br><em>ICLR</em> 2024
              <br>
              <a target="_blank" href="https://openreview.net/pdf?id=6ARlSgun7J">Paper</a> /
              <a target="_blank" href="https://openreview.net/forum?id=6ARlSgun7J">Reviews</a> /
              <a target="_blank" href="https://github.com/anirudhb11/LEVER">Code</a>
              <!-- <p></p>
              <p> We show that latent space based concept detection and removal methods like Null-Space Removal (INLP) and adversarial removal, which internally uses probing classifiers,  are unreliable. They fail to remove the desired concept and, in the worst case, remove or corrupt other features or concepts from the latent space of the classifier.
              </p> -->
            </td>
          </tr>

          <!-- Paper -->
          <tr>
            <td style="padding-top:30px;padding-left:30px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/mdca-banner.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>A Stitch in Time Saves Nine: A Train-Time Regularizing Loss for Improved Neural Network Calibration</papertitle>
              <br>
              <a target="_blank" style="color:black">Ramya Hebbalaguppe*</a>,
							<strong>Jatin Prakash*</strong>,
              <a target="_blank" style="color:black">Neelabh Madan*</a>,
							<a target="_blank" style="color:black">Chetan Arora</a>
              <br><em>CVPR</em> 2022 <font color="red"><strong>Oral</strong></font> <em>(4.2% acceptance rate)</em>
              <br>
              <a target="_blank" href="https://arxiv.org/abs/2203.13834">Paper</a> /
              <a target="_blank" href="https://github.com/mdca-loss/MDCA-Calibration">Code</a>
              <!-- <p></p>
              <p> We show that latent space based concept detection and removal methods like Null-Space Removal (INLP) and adversarial removal, which internally uses probing classifiers,  are unreliable. They fail to remove the desired concept and, in the worst case, remove or corrupt other features or concepts from the latent space of the classifier.
              </p> -->
            </td>
          </tr>

        </tbody></table>
        <footer>
          <p align="right">
            <i>
              You can find my friends <a target="_blank" href="https://aj002.github.io/">here</a>, <a target="_blank" href="https://www.linkedin.com/in/saksham-r-a11a28217/">here</a> and <a target="_blank" href="https://neelabh17.github.io/">here</a></i> üòã
              <i> 
              <br>
              Template inspired from <a target="_blank" href="https://jonbarron.info/">Jon Barron</a>
              
            </i>
          </p>
        </footer>
      </td>
    </tr>
  </table>
</body>



</html>